{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPnQS2kiH6rOdYCIFWv1azC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IzhanAli08/Knowledge_Tracing/blob/main/SAINT_test1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F14oYswfMmPx"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data\n",
        "import torch.nn.utils\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SAKTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, n_skill, max_len=200):\n",
        "        super(SAKTDataset, self).__init__()\n",
        "        self.df = df\n",
        "        self.n_skill = n_skill\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        qids = self.df[0][idx].split(\",\")\n",
        "        qids = [q.strip('\"') for q in qids]\n",
        "\n",
        "        correct = str(self.df[1][idx]).split(\",\")\n",
        "        correct = [c.strip('\"') for c in correct if c]\n",
        "\n",
        "        if len(qids) > self.max_len:\n",
        "            qids = qids[-self.max_len :]\n",
        "            correct = correct[-self.max_len :]\n",
        "\n",
        "        #qids = np.array(list(map(int, qids)))\n",
        "        #correct = np.array(list(map(int, correct)))\n",
        "\n",
        "        # Filter out empty strings and 'nan' before converting to integers\n",
        "        qids = np.array(list(map(int, [q for q in qids if q and q != 'nan'])))\n",
        "        correct = np.array(list(map(int, [c for c in correct if c and c != 'nan'])))\n",
        "\n",
        "\n",
        "        qa = qids + correct * self.n_skill\n",
        "\n",
        "        q = np.ones(self.max_len, dtype=int) * self.n_skill\n",
        "        qa2 = np.ones(self.max_len, dtype=int) * (self.n_skill * 2 + 1)\n",
        "        correct2 = np.ones(self.max_len, dtype=int) * -1\n",
        "        mask = np.zeros(self.max_len, dtype=int)\n",
        "\n",
        "        q[: len(qids)] = qids\n",
        "        qa2[: len(qa)] = qa\n",
        "        correct2[: len(correct)] = correct\n",
        "        mask[: len(qa)] = np.ones(len(qa), dtype=int)\n",
        "\n",
        "        return (\n",
        "            torch.cat(\n",
        "                (torch.LongTensor([2 * self.n_skill]), torch.LongTensor(qa2[:-1]))\n",
        "            ),\n",
        "            torch.LongTensor(q),\n",
        "            torch.LongTensor(correct2),\n",
        "            torch.LongTensor(mask),\n",
        "        )\n",
        "\n",
        "\n",
        "def collate_fn(data, n_skill):\n",
        "    qa = [x[0] for x in data]\n",
        "    qid = [x[1] for x in data]\n",
        "    qc = [x[2] for x in data]\n",
        "    mask = [x[3] for x in data]\n",
        "    qa = torch.nn.utils.rnn.pad_sequence(\n",
        "        qa, batch_first=True, padding_value=n_skill * 2\n",
        "    )\n",
        "    qid = torch.nn.utils.rnn.pad_sequence(qid, batch_first=True, padding_value=n_skill)\n",
        "    qc = torch.nn.utils.rnn.pad_sequence(qc, batch_first=True, padding_value=-1)\n",
        "    mask = torch.nn.utils.rnn.pad_sequence(mask, batch_first=True, padding_value=0)\n",
        "\n",
        "    return qa, qid, qc, mask"
      ],
      "metadata": {
        "id": "0msZ8OSnMvOI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FFN(nn.Module):\n",
        "    def __init__(self, state_size=200, dropout=0.2):\n",
        "        super(FFN, self).__init__()\n",
        "        self.state_size = state_size\n",
        "        self.dropout = dropout\n",
        "        self.lr1 = nn.Linear(self.state_size, self.state_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.lr2 = nn.Linear(self.state_size, self.state_size)\n",
        "        self.dropout = nn.Dropout(self.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lr1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.lr2(x)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "yulTZX9kNAfF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SAKTLoss(nn.Module):\n",
        "    def __init__(self, reduce=\"mean\"):\n",
        "        super(SAKTLoss, self).__init__()\n",
        "        self.reduce = reduce\n",
        "\n",
        "    def forward(self, logits, targets, qid, mask, device=\"cpu\"):\n",
        "\n",
        "        mask = mask.gt(0).view(-1)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        logits = torch.masked_select(logits.view(-1), mask)\n",
        "        targets = torch.masked_select(targets, mask)\n",
        "        loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
        "            logits, targets.float(), reduction=self.reduce\n",
        "        )\n",
        "        return loss\n",
        "def dkt_predict(logits, qid):\n",
        "    preds = torch.sigmoid(logits)\n",
        "    preds = torch.gather(preds, dim=2, index=qid)\n",
        "    preds = torch.squeeze(preds)\n",
        "    binary_preds = torch.round(preds)\n",
        "    return (\n",
        "        preds.view(preds.size()[0], preds.size()[1]),\n",
        "        binary_preds.view(preds.size()[0], preds.size()[1]),\n",
        "    )"
      ],
      "metadata": {
        "id": "urLioXpuNTUS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    roc_auc_score,\n",
        "    precision_recall_fscore_support,\n",
        "    accuracy_score,\n",
        ")\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def train_epoch(model, train_iterator, optim, criterion, device=\"cpu\"):\n",
        "    model.train()\n",
        "\n",
        "    for i, (qa, qid, labels, mask) in enumerate(train_iterator):\n",
        "        qa, qid, labels, mask = (\n",
        "            qa.to(device),\n",
        "            qid.to(device),\n",
        "            labels.to(device),\n",
        "            mask.to(device),\n",
        "        )\n",
        "\n",
        "        optim.zero_grad()\n",
        "        logits, _ = model(qid, qa)\n",
        "        loss = criterion(logits, labels, qid, mask, device=device)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "\n",
        "def eval_epoch(model, test_iterator, criterion, eval_func, device=\"cpu\"):\n",
        "    model.eval()\n",
        "\n",
        "    eval_loss = []\n",
        "    preds, binary_preds, targets = [], [], []\n",
        "    for i, (qa, qid, labels, mask) in enumerate(test_iterator):\n",
        "        qa, qid, labels, mask = (\n",
        "            qa.to(device),\n",
        "            qid.to(device),\n",
        "            labels.to(device),\n",
        "            mask.to(device),\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits, _ = model(qid, qa)\n",
        "\n",
        "        loss = criterion(logits, labels, qid, mask, device=device)\n",
        "        eval_loss.append(loss.detach().item())\n",
        "\n",
        "        mask = mask.eq(1)\n",
        "\n",
        "        # pred, binary_pred = deepkt.loss.dkt_predict(logits, qid)\n",
        "        # pred = torch.masked_select(pred, mask).detach().numpy()\n",
        "        # binary_pred = torch.masked_select(binary_pred, mask).detach().numpy()\n",
        "        # target = torch.masked_select(labels, mask).detach().numpy()\n",
        "        # pred = pred.cpu().detach().numpy().reshape(-1)\n",
        "        # binary_pred = binary_pred.cpu().detach().numpy().reshape(-1)\n",
        "        pred, binary_pred, target = eval_func(logits, qid, labels, mask)\n",
        "        preds.append(pred)\n",
        "        binary_preds.append(binary_pred)\n",
        "        targets.append(target)\n",
        "\n",
        "    preds = np.concatenate(preds)\n",
        "    binary_preds = np.concatenate(binary_preds)\n",
        "    targets = np.concatenate(targets)\n",
        "\n",
        "    auc_value = roc_auc_score(targets, preds)\n",
        "    accuracy = accuracy_score(targets, binary_preds)\n",
        "    precision, recall, f_score, _ = precision_recall_fscore_support(\n",
        "        targets, binary_preds,zero_division=0\n",
        "    )\n",
        "    pos_rate = np.sum(targets) / float(len(targets))\n",
        "    print(\n",
        "        \"auc={0}, accuracy={1}, precision={2}, recall={3}, fscore={4}, pos_rate={5}\".format(\n",
        "            auc_value, accuracy, precision, recall, f_score, pos_rate\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def dkt_eval(logits, qid, targets, mask):\n",
        "    pred, binary_pred = dkt_predict(logits, qid)\n",
        "    pred = torch.masked_select(pred, mask).detach().numpy()\n",
        "    binary_pred = torch.masked_select(binary_pred, mask).detach().numpy()\n",
        "    target = torch.masked_select(targets, mask).detach().numpy()\n",
        "    return pred, binary_pred, target\n",
        "\n",
        "\n",
        "def deepirt_eval(logits, qid, targets, mask):\n",
        "    mask = mask.gt(0).view(-1)\n",
        "    targets = targets.view(-1)\n",
        "\n",
        "    logits = torch.masked_select(logits, mask)\n",
        "\n",
        "    pred = torch.sigmoid(logits).detach().numpy()\n",
        "    binary_pred = pred.round()\n",
        "    target = torch.masked_select(targets, mask).detach().numpy()\n",
        "    return pred, binary_pred, target\n",
        "\n",
        "\n",
        "def sakt_eval(logits, qid, targets, mask):\n",
        "    mask = mask.gt(0).view(-1)\n",
        "    targets = targets.view(-1)\n",
        "\n",
        "    logits = torch.masked_select(logits.view(-1), mask)\n",
        "\n",
        "    pred = torch.sigmoid(logits).cpu().detach().numpy()\n",
        "    binary_pred = pred.round()\n",
        "    #target = torch.masked_select(targets, mask).detach().numpy()\n",
        "    target = torch.masked_select(targets, mask).cpu().detach().numpy() # Moved targets to CPU as well for consistency\n",
        "    return pred, binary_pred, target\n",
        "\n",
        "\n",
        "def future_mask(seq_length):\n",
        "    mask = np.triu(np.ones((seq_length, seq_length)), k=1).astype(\"bool\")\n",
        "    return torch.from_numpy(mask)\n"
      ],
      "metadata": {
        "id": "V_urEME-NsCS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SaintEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim, dropout=0.3, num_heads=4):\n",
        "        super(SaintEncoder, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dropout = dropout\n",
        "        self.num_heads = num_heads\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            embed_dim=self.embed_dim, num_heads=self.num_heads, dropout=self.dropout\n",
        "        )\n",
        "        self.layer_norm1 = nn.LayerNorm(self.embed_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(self.embed_dim)\n",
        "        self.ffn = FFN(self.embed_dim)\n",
        "        self.dropout = nn.Dropout(self.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        x = self.layer_norm1(x)\n",
        "        encoder, _ = self.attn(\n",
        "            x, x, x, attn_mask=future_mask(x.size(0)).to(device)\n",
        "        )\n",
        "        encoder = encoder + x\n",
        "\n",
        "        encoder = encoder.permute(1, 0, 2)\n",
        "        encoder_out = self.layer_norm2(encoder)\n",
        "\n",
        "        return self.ffn(encoder_out) + encoder_out\n",
        "\n",
        "\n",
        "class SaintDecoder(nn.Module):\n",
        "    def __init__(self, embed_dim, dropout=0.3, num_heads=4):\n",
        "        super(SaintDecoder, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dropout = dropout\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.attn1 = nn.MultiheadAttention(\n",
        "            embed_dim=self.embed_dim, num_heads=self.num_heads, dropout=self.dropout\n",
        "        )\n",
        "        self.attn2 = nn.MultiheadAttention(\n",
        "            embed_dim=self.embed_dim, num_heads=self.num_heads, dropout=self.dropout\n",
        "        )\n",
        "\n",
        "        self.ffn = FFN(self.embed_dim)\n",
        "        self.layer_norm1 = nn.LayerNorm(self.embed_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(self.embed_dim)\n",
        "        self.layer_norm3 = nn.LayerNorm(self.embed_dim)\n",
        "\n",
        "    def forward(self, decoder_in, encoder_out):\n",
        "        device = decoder_in.device\n",
        "        x = self.layer_norm1(decoder_in)\n",
        "        decoder, _ = self.attn1(\n",
        "            x, x, x, attn_mask=future_mask(x.size(0)).to(device)\n",
        "        )\n",
        "        decoder = decoder + x\n",
        "\n",
        "        # Reshape encoder_out before passing to attn2\n",
        "        encoder_out = encoder_out.permute(1, 0, 2) # shape: [batch_size, seq_len, embed_dim]\n",
        "        encoder_out = self.layer_norm2(encoder_out)\n",
        "\n",
        "        # Reshape decoder to match encoder_out's shape before passing to attn2\n",
        "        decoder = decoder.permute(1, 0, 2) # shape: [batch_size, seq_len, embed_dim]\n",
        "\n",
        "        encoder_out2 = self.attn2(decoder, encoder_out, encoder_out)[0] # Pass reshaped decoder and encoder_out\n",
        "\n",
        "        decoder_out = decoder + encoder_out2\n",
        "\n",
        "        decoder_out = decoder_out.permute(1, 0, 2)\n",
        "        decoder_out = self.layer_norm3(decoder_out)\n",
        "        return decoder_out + self.ffn(decoder_out)\n",
        "\n",
        "        #encoder_out = encoder_out.permute(1, 0, 2)\n",
        "        #encoder_out = self.layer_norm2(encoder_out)\n",
        "        #encoder_out2 = self.attn2(decoder, encoder_out, encoder_out)[0]\n",
        "        #decoder_out = decoder + encoder_out2\n",
        "\n",
        "        #decoder_out = decoder_out.permute(1, 0, 2)\n",
        "        #decoder_out = self.layer_norm3(decoder_out)\n",
        "        #return decoder_out + self.ffn(decoder_out)\n",
        "\n",
        "\n",
        "class SaintModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_skill,\n",
        "        embed_dim,\n",
        "        dropout,\n",
        "        num_heads=4,\n",
        "        num_enc=4,\n",
        "        max_len=64,\n",
        "        device=\"cpu\",\n",
        "    ):\n",
        "        super(SaintModel, self).__init__()\n",
        "        self.n_skill = n_skill\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dropout = dropout\n",
        "        self.num_heads = num_heads\n",
        "        self.num_enc = num_enc\n",
        "        self.max_len = max_len\n",
        "        self.device = device\n",
        "\n",
        "        self.q_embedding = nn.Embedding(\n",
        "            n_skill + 1, self.embed_dim, padding_idx=n_skill\n",
        "        )\n",
        "        self.qa_embedding = nn.Embedding(\n",
        "            2 * n_skill + 2, self.embed_dim, padding_idx=2 * n_skill + 1\n",
        "        )\n",
        "        self.pos_embedding = nn.Embedding(self.max_len, self.embed_dim)\n",
        "\n",
        "        self.encoders = nn.ModuleList(\n",
        "            [\n",
        "                SaintEncoder(self.embed_dim, self.dropout, self.num_heads)\n",
        "                for x in range(self.num_enc)\n",
        "            ]\n",
        "        )\n",
        "        self.decoders = nn.ModuleList(\n",
        "            [\n",
        "                SaintDecoder(self.embed_dim, self.dropout, self.num_heads)\n",
        "                for x in range(self.num_enc)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(self.embed_dim, 1)\n",
        "\n",
        "    def forward(self, q, qa):\n",
        "        qa = self.qa_embedding(qa)\n",
        "        pos_id = torch.arange(qa.size(1)).unsqueeze(0).to(self.device)\n",
        "        pos_x = self.pos_embedding(pos_id)\n",
        "        qa = qa + pos_x\n",
        "        q = self.q_embedding(q)\n",
        "\n",
        "        q = q.permute(1, 0, 2)\n",
        "        qa = qa.permute(1, 0, 2)\n",
        "\n",
        "        for x in range(self.num_enc):\n",
        "            q = self.encoders[x](q)\n",
        "\n",
        "        for x in range(self.num_enc):\n",
        "            qa = self.decoders[x](qa, q)\n",
        "\n",
        "        logits = self.fc(qa)\n",
        "        return logits, None\n"
      ],
      "metadata": {
        "id": "qGF3bnM8N8hB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "sys.path.insert(0, \"..\")\n",
        "\n",
        "import argparse\n",
        "import torch\n",
        "import torch.optim\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import logging\n",
        "# Import the collate_fn\n",
        "from __main__ import collate_fn\n",
        "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.INFO)\n",
        "\n",
        "\n",
        "def run(args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_df = pd.read_csv(\"/content/assist2009_train.csv\",\n",
        "                           header=None,\n",
        "                           sep=',')\n",
        "    test_df = pd.read_csv(\"/content/assist2009_test.csv\", header=None, sep=',')\n",
        "\n",
        "    train = SAKTDataset(train_df, args.num_skill, max_len=100)\n",
        "    test = SAKTDataset(test_df, args.num_skill, max_len=100)\n",
        "    train_dataloader = DataLoader(train,\n",
        "                                  batch_size=args.batch_size,\n",
        "                                  num_workers=args.num_worker,\n",
        "                                  shuffle=True)\n",
        "    test_dataloader = DataLoader(test,\n",
        "                                 batch_size=args.batch_size * 2,\n",
        "                                 num_workers=args.num_worker,\n",
        "                                 shuffle=False)\n",
        "\n",
        "    saint = SaintModel(args.num_skill, args.embed_dim, args.dropout, args.num_heads,\n",
        "                       args.num_enc, device=device, max_len=100)\n",
        "\n",
        "    optimizer = torch.optim.Adam(saint.parameters(), lr=args.learning_rate)\n",
        "    loss_func = SAKTLoss()\n",
        "\n",
        "    saint.to(device)\n",
        "    loss_func.to(device)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
        "    for epoch in range(args.epoch):\n",
        "        train_epoch(saint, train_dataloader, optimizer, loss_func,\n",
        "                                 device)\n",
        "        eval_epoch(saint, test_dataloader, loss_func, sakt_eval, device)\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    arg_parser = argparse.ArgumentParser(description=\"train deep IRT model\")\n",
        "    # Provide default values for the arguments\n",
        "    arg_parser.add_argument(\"--learning_rate\", dest=\"learning_rate\", default=0.001, type=float)\n",
        "    arg_parser.add_argument(\"--batch_size\", dest=\"batch_size\", default=64, type=int)\n",
        "    arg_parser.add_argument(\"--num_skill\", dest=\"num_skill\", default=150, type=int)\n",
        "    arg_parser.add_argument(\"--embed_dim\", dest=\"embed_dim\", default=200, type=int)\n",
        "    arg_parser.add_argument(\"--dropout\", dest=\"dropout\", default=0.2, type=float)\n",
        "    arg_parser.add_argument(\"--num_heads\", dest=\"num_heads\", default=5, type=int)\n",
        "    arg_parser.add_argument(\"--epoch\", dest=\"epoch\", default=10, type=int)\n",
        "    arg_parser.add_argument(\"--num_worker\", dest=\"num_worker\", default=0, type=int)\n",
        "    # Define the num_enc argument with a default value\n",
        "    arg_parser.add_argument(\"--num_enc\", dest=\"num_enc\", default=4, type=int) # This line is added to define the num_enc argument\n",
        "\n",
        "    args = arg_parser.parse_args([]) #remove any other code that is being passed to parse_args such as empty lists.\n",
        "    run(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDSBntAFOeO6",
        "outputId": "b37ad15f-9d47-43d3-d210-ea8586a1803c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "auc=0.5018910084607004, accuracy=0.6307840616966581, precision=[0.         0.63078406], recall=[0. 1.], fscore=[0.         0.77359606], pos_rate=0.6307840616966581\n",
            "auc=0.4970294973990096, accuracy=0.6307840616966581, precision=[0.         0.63078406], recall=[0. 1.], fscore=[0.         0.77359606], pos_rate=0.6307840616966581\n",
            "auc=0.49744180968456037, accuracy=0.6307840616966581, precision=[0.         0.63078406], recall=[0. 1.], fscore=[0.         0.77359606], pos_rate=0.6307840616966581\n",
            "auc=0.4912536760353751, accuracy=0.6307840616966581, precision=[0.         0.63078406], recall=[0. 1.], fscore=[0.         0.77359606], pos_rate=0.6307840616966581\n",
            "auc=0.4906633223778279, accuracy=0.6307840616966581, precision=[0.         0.63078406], recall=[0. 1.], fscore=[0.         0.77359606], pos_rate=0.6307840616966581\n",
            "auc=0.4900040146540415, accuracy=0.6307840616966581, precision=[0.         0.63078406], recall=[0. 1.], fscore=[0.         0.77359606], pos_rate=0.6307840616966581\n",
            "auc=0.49099409129824295, accuracy=0.6307840616966581, precision=[0.         0.63078406], recall=[0. 1.], fscore=[0.         0.77359606], pos_rate=0.6307840616966581\n",
            "auc=0.4924183136502228, accuracy=0.6307840616966581, precision=[0.         0.63078406], recall=[0. 1.], fscore=[0.         0.77359606], pos_rate=0.6307840616966581\n",
            "auc=0.49095505316590166, accuracy=0.6307840616966581, precision=[0.         0.63078406], recall=[0. 1.], fscore=[0.         0.77359606], pos_rate=0.6307840616966581\n",
            "auc=0.4912687902878624, accuracy=0.6307840616966581, precision=[0.         0.63078406], recall=[0. 1.], fscore=[0.         0.77359606], pos_rate=0.6307840616966581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "import torch.nn as nn\n",
        "\n",
        "def measure_memory(model, input_q, input_qa):\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    model.cuda()\n",
        "    input_q = input_q.cuda()\n",
        "    input_qa = input_qa.cuda()\n",
        "\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(input_q, input_qa)\n",
        "\n",
        "    max_mem = torch.cuda.max_memory_allocated() / (1024 ** 2)  # in MB\n",
        "    print(f\"Max memory used: {max_mem:.2f} MB\")\n",
        "    return max_mem\n",
        "\n",
        "# Define parameters for the SaintModel instance\n",
        "n_skill = 150  # Example value, adjust based on your data\n",
        "embed_dim = 100\n",
        "dropout = 0.001\n",
        "num_heads = 10\n",
        "max_len = 128 # Max sequence length\n",
        "batch_size = 64 # Use a small batch size for memory measurement\n",
        "\n",
        "# Create an instance of SaintModel\n",
        "# You also need to provide device if not default\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "sakt_model_instance = SaintModel(n_skill, embed_dim, dropout, num_heads, max_len=max_len, device=device)\n",
        "\n",
        "# Create dummy input tensors with appropriate dimensions (batch_size, sequence_length)\n",
        "# The SAKTModel forward method expects q and qa as inputs.\n",
        "# q is typically the question sequence, qa is the question-answer sequence.\n",
        "# The values should be within the range of your embedding layers' vocabulary size.\n",
        "# For q, values should be < n_skill + 1.\n",
        "# For qa, values should be < 2 * n_skill + 2.\n",
        "dummy_input_q = torch.randint(0, n_skill, (batch_size, max_len), dtype=torch.long)\n",
        "dummy_input_qa = torch.randint(0, 2 * n_skill + 1, (batch_size, max_len), dtype=torch.long)\n",
        "\n",
        "\n",
        "# Pass the instance and dummy inputs to the measure_memory function\n",
        "mem_dot = measure_memory(sakt_model_instance, dummy_input_q, dummy_input_qa)\n",
        "\n",
        "# Correct the typo in print\n",
        "print(mem_dot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohMkCK8OtYTU",
        "outputId": "4188faa8-5c55-4de8-9d61-92f84e59b5c5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max memory used: 131.85 MB\n",
            "131.8505859375\n"
          ]
        }
      ]
    }
  ]
}