{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyN6Na29ZSYLFSIt5Ny3CDLe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IzhanAli08/Knowledge_Tracing/blob/main/LinSAKT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F14oYswfMmPx"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data\n",
        "import torch.nn.utils\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SAKTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, n_skill, max_len=200):\n",
        "        super(SAKTDataset, self).__init__()\n",
        "        self.df = df\n",
        "        self.n_skill = n_skill\n",
        "        self.max_len = max_len\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        qids = self.df[0][idx].split(\",\")\n",
        "        qids = [q.strip('\"') for q in qids]\n",
        "\n",
        "        correct = str(self.df[1][idx]).split(\",\")\n",
        "        correct = [c.strip('\"') for c in correct if c]\n",
        "\n",
        "        if len(qids) > self.max_len:\n",
        "            qids = qids[-self.max_len :]\n",
        "            correct = correct[-self.max_len :]\n",
        "\n",
        "        #qids = np.array(list(map(int, qids)))\n",
        "        #correct = np.array(list(map(int, correct)))\n",
        "\n",
        "        # Filter out empty strings and 'nan' before converting to integers\n",
        "        qids = np.array(list(map(int, [q for q in qids if q and q != 'nan'])))\n",
        "        correct = np.array(list(map(int, [c for c in correct if c and c != 'nan'])))\n",
        "\n",
        "        # adding for 2009 Ensure correct and qids have the same length before combining, pad or truncate if necessary\n",
        "        min_len = min(len(qids), len(correct))\n",
        "        qids = qids[:min_len]\n",
        "        correct = correct[:min_len]\n",
        "\n",
        "\n",
        "        qa = qids + correct * self.n_skill\n",
        "\n",
        "         # --- Add assertions here to check the range of generated indices ---\n",
        "        # Check qids values for q_embedding\n",
        "        assert np.all(qids >= 0) and np.all(qids < self.n_skill), \\\n",
        "            f\"Invalid qid index found at index {idx}. Values must be between 0 and {self.n_skill - 1}. Found: {qids}\"\n",
        "\n",
        "        # Check qa values for qa_embedding\n",
        "        # The qa_embedding is defined with size 2*n_skill + 2, with padding_idx = 2*n_skill + 1.\n",
        "        # Valid indices are 0 to 2*n_skill.\n",
        "        valid_qa_max = 2 * self.n_skill\n",
        "        assert np.all(qa >= 0) and np.all(qa <= valid_qa_max), \\\n",
        "            f\"Invalid qa index found at index {idx}. Values must be between 0 and {valid_qa_max}. Found: {qa}\"\n",
        "        # --- End of assertions ---\n",
        "\n",
        "        q = np.ones(self.max_len, dtype=int) * self.n_skill\n",
        "        qa2 = np.ones(self.max_len, dtype=int) * (self.n_skill * 2 + 1)\n",
        "        correct2 = np.ones(self.max_len, dtype=int) * -1\n",
        "        mask = np.zeros(self.max_len, dtype=int)\n",
        "\n",
        "        q[: len(qids)] = qids\n",
        "        qa2[: len(qa)] = qa\n",
        "        correct2[: len(correct)] = correct\n",
        "        mask[: len(qa)] = np.ones(len(qa), dtype=int)\n",
        "\n",
        "        return (\n",
        "            torch.cat(\n",
        "                (torch.LongTensor([2 * self.n_skill]), torch.LongTensor(qa2[:-1]))\n",
        "            ),\n",
        "            torch.LongTensor(q),\n",
        "            torch.LongTensor(correct2),\n",
        "            torch.LongTensor(mask),\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "def collate_fn(data, n_skill):\n",
        "    qa = [x[0] for x in data]\n",
        "    qid = [x[1] for x in data]\n",
        "    qc = [x[2] for x in data]\n",
        "    mask = [x[3] for x in data]\n",
        "    qa = torch.nn.utils.rnn.pad_sequence(\n",
        "        qa, batch_first=True, padding_value=n_skill * 2\n",
        "    )\n",
        "    qid = torch.nn.utils.rnn.pad_sequence(qid, batch_first=True, padding_value=n_skill)\n",
        "    qc = torch.nn.utils.rnn.pad_sequence(qc, batch_first=True, padding_value=-1)\n",
        "    mask = torch.nn.utils.rnn.pad_sequence(mask, batch_first=True, padding_value=0)\n",
        "\n",
        "    return qa, qid, qc, mask"
      ],
      "metadata": {
        "id": "0msZ8OSnMvOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FFN(nn.Module):\n",
        "    def __init__(self, state_size=200, dropout=0.2):\n",
        "        super(FFN, self).__init__()\n",
        "        self.state_size = state_size\n",
        "        self.dropout = dropout\n",
        "        self.lr1 = nn.Linear(self.state_size, self.state_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.lr2 = nn.Linear(self.state_size, self.state_size)\n",
        "        self.dropout = nn.Dropout(self.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lr1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.lr2(x)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "yulTZX9kNAfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CausalLinearAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def _feature_map(self, x):\n",
        "        return F.elu(x) + 1  # Positive kernel\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        B, T, _ = query.shape\n",
        "        H = self.num_heads\n",
        "        D = self.head_dim\n",
        "\n",
        "        # Linear projections\n",
        "        #Q = self.q_proj(query).reshape(B, T, H, D)\n",
        "        Q = self.q_proj(query).view(B, T, H, D).contiguous()\n",
        "        #K = self.k_proj(key).reshape(B, T, H, D)\n",
        "        K = self.k_proj(key).view(B, T, H, D).contiguous()\n",
        "        #V = self.v_proj(value).reshape(B, T, H, D)\n",
        "        V = self.v_proj(value).view(B, T, H, D).contiguous()\n",
        "\n",
        "        Q = self._feature_map(Q)\n",
        "        K = self._feature_map(K)\n",
        "\n",
        "        # Causal cumulative KV\n",
        "        K_cum = torch.cumsum(K, dim=1)  # B, T, H, D\n",
        "        KV_cum = torch.cumsum(K * V, dim=1)  # B, T, H, D\n",
        "\n",
        "        #Z = 1 / (torch.einsum(\"bthd,bthd->bth\", Q, K_cum) + 1e-6)\n",
        "        Z = 1 / ((Q * K_cum).sum(dim=-1) + 1e-6)\n",
        "        #context = torch.einsum(\"bthd,bthd->bthd\", Q, KV_cum)\n",
        "        context = Q * KV_cum\n",
        "        output = context * Z.unsqueeze(-1)\n",
        "\n",
        "        output = output.reshape(B, T, self.embed_dim)\n",
        "        return self.out_proj(output)\n"
      ],
      "metadata": {
        "id": "FXvNHcmy1dFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalLinearAttention_new(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def _feature_map(self, x):\n",
        "        return F.relu(x) + 1  # Revert to original ELU\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        B, T, _ = query.shape\n",
        "        H, D = self.num_heads, self.head_dim\n",
        "        Q = self._feature_map(self.q_proj(query).view(B, T, H, D))\n",
        "        K = self._feature_map(self.k_proj(key).view(B, T, H, D))\n",
        "        V = self.v_proj(value).view(B, T, H, D)\n",
        "        window_size = min(32, T)  # Sliding window\n",
        "        K_cum = torch.cumsum(K[:, -window_size:], dim=1)\n",
        "        KV_cum = torch.cumsum((K * V)[:, -window_size:], dim=1)\n",
        "        output = Q[:, -window_size:] * KV_cum / ((Q[:, -window_size:] * K_cum).sum(dim=-1, keepdim=True) + 1e-6)\n",
        "        output = F.pad(output, (0, 0, 0, 0, T - window_size, 0), value=0)\n",
        "        output = output.view(B, T, self.embed_dim)\n",
        "        return self.out_proj(output)"
      ],
      "metadata": {
        "id": "7bQ842dIWC2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SAKTLoss(nn.Module):\n",
        "    def __init__(self, reduce=\"mean\"):\n",
        "        super(SAKTLoss, self).__init__()\n",
        "        self.reduce = reduce\n",
        "\n",
        "    def forward(self, logits, targets, qid, mask, device=\"cpu\"):\n",
        "\n",
        "        mask = mask.gt(0).view(-1)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        logits = torch.masked_select(logits.view(-1), mask)\n",
        "        targets = torch.masked_select(targets, mask)\n",
        "        loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
        "            logits, targets.float(), reduction=self.reduce\n",
        "        )\n",
        "        return loss\n",
        "def dkt_predict(logits, qid):\n",
        "    preds = torch.sigmoid(logits)\n",
        "    preds = torch.gather(preds, dim=2, index=qid)\n",
        "    preds = torch.squeeze(preds)\n",
        "    binary_preds = torch.round(preds)\n",
        "    return (\n",
        "        preds.view(preds.size()[0], preds.size()[1]),\n",
        "        binary_preds.view(preds.size()[0], preds.size()[1]),\n",
        "    )"
      ],
      "metadata": {
        "id": "urLioXpuNTUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    roc_auc_score,\n",
        "    precision_recall_fscore_support,\n",
        "    accuracy_score,\n",
        ")\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def train_epoch(model, train_iterator, optim, criterion, device=\"cpu\"):\n",
        "    model.train()\n",
        "\n",
        "    for i, (qa, qid, labels, mask) in enumerate(train_iterator):\n",
        "        qa, qid, labels, mask = (\n",
        "            qa.to(device),\n",
        "            qid.to(device),\n",
        "            labels.to(device),\n",
        "            mask.to(device),\n",
        "        )\n",
        "\n",
        "        optim.zero_grad()\n",
        "        logits, _ = model(qid, qa)\n",
        "        loss = criterion(logits, labels, qid, mask, device=device)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "\n",
        "def eval_epoch(model, test_iterator, criterion, eval_func, device=\"cpu\"):\n",
        "    model.eval()\n",
        "\n",
        "    eval_loss = []\n",
        "    preds, binary_preds, targets = [], [], []\n",
        "    for i, (qa, qid, labels, mask) in enumerate(test_iterator):\n",
        "        qa, qid, labels, mask = (\n",
        "            qa.to(device),\n",
        "            qid.to(device),\n",
        "            labels.to(device),\n",
        "            mask.to(device),\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits, _ = model(qid, qa)\n",
        "\n",
        "        loss = criterion(logits, labels, qid, mask, device=device)\n",
        "        eval_loss.append(loss.detach().item())\n",
        "\n",
        "        mask = mask.eq(1)\n",
        "\n",
        "        # pred, binary_pred = deepkt.loss.dkt_predict(logits, qid)\n",
        "        # pred = torch.masked_select(pred, mask).detach().numpy()\n",
        "        # binary_pred = torch.masked_select(binary_pred, mask).detach().numpy()\n",
        "        # target = torch.masked_select(labels, mask).detach().numpy()\n",
        "        # pred = pred.cpu().detach().numpy().reshape(-1)\n",
        "        # binary_pred = binary_pred.cpu().detach().numpy().reshape(-1)\n",
        "        pred, binary_pred, target = eval_func(logits, qid, labels, mask)\n",
        "        preds.append(pred)\n",
        "        binary_preds.append(binary_pred)\n",
        "        targets.append(target)\n",
        "\n",
        "    preds = np.concatenate(preds)\n",
        "    binary_preds = np.concatenate(binary_preds)\n",
        "    targets = np.concatenate(targets)\n",
        "\n",
        "    auc_value = roc_auc_score(targets, preds)\n",
        "    accuracy = accuracy_score(targets, binary_preds)\n",
        "    precision, recall, f_score, _ = precision_recall_fscore_support(\n",
        "        targets, binary_preds\n",
        "    )\n",
        "    pos_rate = np.sum(targets) / float(len(targets))\n",
        "    print(\n",
        "        \"auc={0}, accuracy={1}, precision={2}, recall={3}, fscore={4}, pos_rate={5}\".format(\n",
        "            auc_value, accuracy, precision, recall, f_score, pos_rate\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def dkt_eval(logits, qid, targets, mask):\n",
        "    pred, binary_pred = dkt_predict(logits, qid)\n",
        "    pred = torch.masked_select(pred, mask).detach().numpy()\n",
        "    binary_pred = torch.masked_select(binary_pred, mask).detach().numpy()\n",
        "    target = torch.masked_select(targets, mask).detach().numpy()\n",
        "    return pred, binary_pred, target\n",
        "\n",
        "\n",
        "def deepirt_eval(logits, qid, targets, mask):\n",
        "    mask = mask.gt(0).view(-1)\n",
        "    targets = targets.view(-1)\n",
        "\n",
        "    logits = torch.masked_select(logits, mask)\n",
        "\n",
        "    pred = torch.sigmoid(logits).detach().numpy()\n",
        "    binary_pred = pred.round()\n",
        "    target = torch.masked_select(targets, mask).detach().numpy()\n",
        "    return pred, binary_pred, target\n",
        "\n",
        "\n",
        "def sakt_eval(logits, qid, targets, mask):\n",
        "    mask = mask.gt(0).view(-1)\n",
        "    targets = targets.view(-1)\n",
        "\n",
        "    logits = torch.masked_select(logits.view(-1), mask)\n",
        "\n",
        "    pred = torch.sigmoid(logits).cpu().detach().numpy()\n",
        "    binary_pred = pred.round()\n",
        "    #target = torch.masked_select(targets, mask).detach().numpy()\n",
        "    target = torch.masked_select(targets, mask).cpu().detach().numpy() # Moved targets to CPU as well for consistency\n",
        "    return pred, binary_pred, target\n",
        "\n",
        "\n",
        "def future_mask(seq_length):\n",
        "    mask = np.triu(np.ones((seq_length, seq_length)), k=1).astype(\"bool\")\n",
        "    return torch.from_numpy(mask)\n"
      ],
      "metadata": {
        "id": "V_urEME-NsCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SAKTModel(nn.Module):\n",
        "    def __init__(\n",
        "        self, n_skill, embed_dim, dropout, num_heads=4, max_len=64, device=\"cpu\"\n",
        "    ):\n",
        "        super(SAKTModel, self).__init__()\n",
        "        self.n_skill = n_skill\n",
        "        self.q_embed_dim = embed_dim\n",
        "        self.qa_embed_dim = embed_dim\n",
        "        self.pos_embed_dim = embed_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dropout = dropout\n",
        "        self.num_heads = num_heads\n",
        "        self.max_len = max_len\n",
        "        self.device = device\n",
        "\n",
        "        self.q_embedding = nn.Embedding(\n",
        "            n_skill + 1, self.q_embed_dim, padding_idx=n_skill\n",
        "        )\n",
        "        self.qa_embedding = nn.Embedding(\n",
        "            2 * n_skill + 2, self.qa_embed_dim, padding_idx=2 * n_skill + 1\n",
        "        )\n",
        "        self.pos_embedding = nn.Embedding(self.max_len, self.pos_embed_dim)\n",
        "\n",
        "        #self.multi_attention = nn.MultiheadAttention(\n",
        "            #embed_dim=self.embed_dim, num_heads=self.num_heads, dropout=self.dropout\n",
        "        #)\n",
        "        self.linear_attention = CausalLinearAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
        "\n",
        "        self.layer_norm1 = nn.LayerNorm(self.embed_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(self.embed_dim)\n",
        "        self.dropout_layer = nn.Dropout(self.dropout)\n",
        "        self.ffn = FFN(self.embed_dim)\n",
        "        self.pred = nn.Linear(self.embed_dim, 1, bias=True)\n",
        "\n",
        "    def forward(self, q, qa):\n",
        "        qa = self.qa_embedding(qa)\n",
        "        pos_id = torch.arange(qa.size(1), device=self.device).unsqueeze(0)\n",
        "        qa += self.pos_embedding(pos_id)\n",
        "        q = self.q_embedding(q)\n",
        "\n",
        "        attention_out = self.linear_attention(q, qa, qa)\n",
        "        attention_out = self.layer_norm1(attention_out + q)\n",
        "\n",
        "        x = self.ffn(attention_out)\n",
        "        x = self.dropout_layer(x)\n",
        "        x = self.layer_norm2(x + attention_out)\n",
        "        x = self.pred(x)\n",
        "\n",
        "        return x.squeeze(-1), None\n"
      ],
      "metadata": {
        "id": "qGF3bnM8N8hB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import time\n",
        "\n",
        "sys.path.insert(0, \"..\")\n",
        "\n",
        "import argparse\n",
        "import torch\n",
        "import torch.optim\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.INFO)\n",
        "\n",
        "\n",
        "def run(args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_df = pd.read_csv(\"/content/assist2015_train.csv\",\n",
        "                           header=None,\n",
        "                           sep='\\t')\n",
        "    test_df = pd.read_csv(\"/content/assist2015_test.csv\", header=None, sep='\\t')\n",
        "\n",
        "    train = SAKTDataset(train_df, args.num_skill, max_len=64)\n",
        "    test = SAKTDataset(test_df, args.num_skill, max_len=64)\n",
        "    train_dataloader = DataLoader(train,\n",
        "                                  batch_size=args.batch_size,\n",
        "                                  num_workers=args.num_worker,\n",
        "                                  shuffle=True)\n",
        "    test_dataloader = DataLoader(test,\n",
        "                                 batch_size=args.batch_size * 2,\n",
        "                                 num_workers=args.num_worker,\n",
        "                                 shuffle=False)\n",
        "\n",
        "    sakt = SAKTModel(args.num_skill, args.embed_dim, args.dropout, args.num_heads, device=device, max_len=64)\n",
        "\n",
        "    optimizer = torch.optim.Adam(sakt.parameters(), lr=args.learning_rate)\n",
        "    loss_func = SAKTLoss()\n",
        "\n",
        "    sakt.to(device)\n",
        "    loss_func.to(device)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
        "    start_time = time.time()\n",
        "    for epoch in range(args.epoch):\n",
        "        train_epoch(sakt, train_dataloader, optimizer, loss_func,\n",
        "                                 device)\n",
        "        eval_epoch(sakt, test_dataloader, loss_func, sakt_eval, device)\n",
        "        scheduler.step()\n",
        "    print(\"total time: \", time.time() - start_time, \"seconds\")\n",
        "    print('average time per epoch', (time.time() - start_time) / args.epoch)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    arg_parser = argparse.ArgumentParser(description=\"train deep IRT model\")\n",
        "    # Provide default values for the arguments\n",
        "    arg_parser.add_argument(\"--learning_rate\", dest=\"learning_rate\", default=0.001, type=float)\n",
        "    arg_parser.add_argument(\"--batch_size\", dest=\"batch_size\", default=64, type=int)\n",
        "    arg_parser.add_argument(\"--num_skill\", dest=\"num_skill\", default=230, type=int)\n",
        "    arg_parser.add_argument(\"--embed_dim\", dest=\"embed_dim\", default=256, type=int)\n",
        "    arg_parser.add_argument(\"--dropout\", dest=\"dropout\", default=0.2, type=float)\n",
        "    arg_parser.add_argument(\"--num_heads\", dest=\"num_heads\", default=4, type=int)\n",
        "    arg_parser.add_argument(\"--epoch\", dest=\"epoch\", default=10, type=int)\n",
        "    arg_parser.add_argument(\"--num_worker\", dest=\"num_worker\", default=0, type=int)\n",
        "\n",
        "    args = arg_parser.parse_args([]) #remove any other code that is being passed to parse_args such as empty lists.\n",
        "    run(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDSBntAFOeO6",
        "outputId": "3c2fbf80-721d-46ae-c023-8f2f0b474e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "auc=0.7081011881813667, accuracy=0.7581930499048187, precision=[0.58038924 0.77266718], recall=[0.17206909 0.95766316], fscore=[0.26544207 0.85527587], pos_rate=0.7460901048859692\n",
            "auc=0.7174931992088027, accuracy=0.7593688178866037, precision=[0.57366187 0.77776524], recall=[0.20363837 0.94849538], fscore=[0.30057772 0.85468756], pos_rate=0.7460901048859692\n",
            "auc=0.7208239491474595, accuracy=0.7603206300623344, precision=[0.58456249 0.77646795], recall=[0.19371555 0.95314806], fscore=[0.29099843 0.85578408], pos_rate=0.7460901048859692\n",
            "auc=0.7235542647147063, accuracy=0.7600873427643612, precision=[0.57597245 0.77876907], recall=[0.20896729 0.9476449 ], fscore=[0.3066717  0.85494736], pos_rate=0.7460901048859692\n",
            "auc=0.722532045185445, accuracy=0.7602553096189019, precision=[0.57949309 0.77793599], recall=[0.20334436 0.94978363], fscore=[0.30105011 0.8553134 ], pos_rate=0.7460901048859692\n",
            "auc=0.722784855472345, accuracy=0.7600406853047665, precision=[0.58131187 0.77681151], recall=[0.19639838 0.95185982], fscore=[0.29360218 0.85547281], pos_rate=0.7460901048859692\n",
            "auc=0.7230860625234372, accuracy=0.7602459781269829, precision=[0.59515745 0.77351224], recall=[0.17434767 0.95963929], fscore=[0.26969103 0.85658147], pos_rate=0.7460901048859692\n",
            "auc=0.7224656472124171, accuracy=0.7590888731290358, precision=[0.56827762 0.77916319], recall=[0.21304667 0.94491833], fscore=[0.30990885 0.85407281], pos_rate=0.7460901048859692\n",
            "auc=0.7216090286920491, accuracy=0.7594061438542794, precision=[0.56727959 0.78050725], recall=[0.22109519 0.9426045 ], fscore=[0.31818062 0.85393144], pos_rate=0.7460901048859692\n",
            "auc=0.7213966020171264, accuracy=0.7589675637340898, precision=[0.5664868  0.77960655], recall=[0.21606027 0.94373014], fscore=[0.3128126  0.85385312], pos_rate=0.7460901048859692\n",
            "total time:  47.028756618499756 seconds\n",
            "average time per epoch 4.702877497673034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "import torch.nn as nn\n",
        "\n",
        "def measure_memory(model, input_q, input_qa):\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    model.cuda()\n",
        "    input_q = input_q.cuda()\n",
        "    input_qa = input_qa.cuda()\n",
        "\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(input_q, input_qa)\n",
        "\n",
        "    max_mem = torch.cuda.max_memory_allocated() / (1024 ** 2)  # in MB\n",
        "    print(f\"Max memory used: {max_mem:.2f} MB\")\n",
        "    return max_mem\n",
        "\n",
        "# Define parameters for the SAKTModel instance\n",
        "n_skill = 230  # Example value, adjust based on your data\n",
        "embed_dim = 256\n",
        "dropout = 0.001\n",
        "num_heads = 4\n",
        "max_len = 512 # Max sequence length\n",
        "batch_size = 64\n",
        "\n",
        "# Create an instance of SAKTModel\n",
        "# You also need to provide device if not default\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "sakt_model_instance = SAKTModel(n_skill, embed_dim, dropout, num_heads, max_len=max_len, device=device)\n",
        "\n",
        "# Create dummy input tensors with appropriate dimensions (batch_size, sequence_length)\n",
        "# The SAKTModel forward method expects q and qa as inputs.\n",
        "# q is typically the question sequence, qa is the question-answer sequence.\n",
        "# The values should be within the range of your embedding layers' vocabulary size.\n",
        "# For q, values should be < n_skill + 1.\n",
        "# For qa, values should be < 2 * n_skill + 2.\n",
        "dummy_input_q = torch.randint(0, n_skill, (batch_size, max_len), dtype=torch.long)\n",
        "dummy_input_qa = torch.randint(0, 2 * n_skill + 1, (batch_size, max_len), dtype=torch.long)\n",
        "\n",
        "\n",
        "# Pass the instance and dummy inputs to the measure_memory function\n",
        "mem_linear = measure_memory(sakt_model_instance, dummy_input_q, dummy_input_qa)\n",
        "\n",
        "# Correct the typo in print\n",
        "print(mem_linear)"
      ],
      "metadata": {
        "id": "pveAfYqlWjqm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd4011b1-d3d1-4879-c79b-e7ff7bc2bc69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max memory used: 340.94 MB\n",
            "340.94189453125\n"
          ]
        }
      ]
    }
  ]
}