{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOkqBoZ0R34TguRW99We3G4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IzhanAli08/Knowledge_Tracing/blob/main/SAKT_test1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F14oYswfMmPx"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data\n",
        "import torch.nn.utils\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SAKTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, n_skill, max_len=200):\n",
        "        super(SAKTDataset, self).__init__()\n",
        "        self.df = df\n",
        "        self.n_skill = n_skill\n",
        "        self.max_len = max_len\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        qids = self.df[0][idx].split(\",\")\n",
        "        qids = [q.strip('\"') for q in qids]\n",
        "\n",
        "        correct = str(self.df[1][idx]).split(\",\")\n",
        "        correct = [c.strip('\"') for c in correct if c]\n",
        "\n",
        "        if len(qids) > self.max_len:\n",
        "            qids = qids[-self.max_len :]\n",
        "            correct = correct[-self.max_len :]\n",
        "\n",
        "        #qids = np.array(list(map(int, qids)))\n",
        "        #correct = np.array(list(map(int, correct)))\n",
        "\n",
        "        # Filter out empty strings and 'nan' before converting to integers\n",
        "        qids = np.array(list(map(int, [q for q in qids if q and q != 'nan'])))\n",
        "        correct = np.array(list(map(int, [c for c in correct if c and c != 'nan'])))\n",
        "\n",
        "        # adding for 2009 Ensure correct and qids have the same length before combining, pad or truncate if necessary\n",
        "        min_len = min(len(qids), len(correct))\n",
        "        qids = qids[:min_len]\n",
        "        correct = correct[:min_len]\n",
        "\n",
        "\n",
        "        qa = qids + correct * self.n_skill\n",
        "\n",
        "         # --- Add assertions here to check the range of generated indices ---\n",
        "        # Check qids values for q_embedding\n",
        "        assert np.all(qids >= 0) and np.all(qids < self.n_skill), \\\n",
        "            f\"Invalid qid index found at index {idx}. Values must be between 0 and {self.n_skill - 1}. Found: {qids}\"\n",
        "\n",
        "        # Check qa values for qa_embedding\n",
        "        # The qa_embedding is defined with size 2*n_skill + 2, with padding_idx = 2*n_skill + 1.\n",
        "        # Valid indices are 0 to 2*n_skill.\n",
        "        valid_qa_max = 2 * self.n_skill\n",
        "        assert np.all(qa >= 0) and np.all(qa <= valid_qa_max), \\\n",
        "            f\"Invalid qa index found at index {idx}. Values must be between 0 and {valid_qa_max}. Found: {qa}\"\n",
        "        # --- End of assertions ---\n",
        "\n",
        "        q = np.ones(self.max_len, dtype=int) * self.n_skill\n",
        "        qa2 = np.ones(self.max_len, dtype=int) * (self.n_skill * 2 + 1)\n",
        "        correct2 = np.ones(self.max_len, dtype=int) * -1\n",
        "        mask = np.zeros(self.max_len, dtype=int)\n",
        "\n",
        "        q[: len(qids)] = qids\n",
        "        qa2[: len(qa)] = qa\n",
        "        correct2[: len(correct)] = correct\n",
        "        mask[: len(qa)] = np.ones(len(qa), dtype=int)\n",
        "\n",
        "        return (\n",
        "            torch.cat(\n",
        "                (torch.LongTensor([2 * self.n_skill]), torch.LongTensor(qa2[:-1]))\n",
        "            ),\n",
        "            torch.LongTensor(q),\n",
        "            torch.LongTensor(correct2),\n",
        "            torch.LongTensor(mask),\n",
        "        )\n",
        "\n",
        "\n",
        "def collate_fn(data, n_skill):\n",
        "    qa = [x[0] for x in data]\n",
        "    qid = [x[1] for x in data]\n",
        "    qc = [x[2] for x in data]\n",
        "    mask = [x[3] for x in data]\n",
        "    qa = torch.nn.utils.rnn.pad_sequence(\n",
        "        qa, batch_first=True, padding_value=n_skill * 2\n",
        "    )\n",
        "    qid = torch.nn.utils.rnn.pad_sequence(qid, batch_first=True, padding_value=n_skill)\n",
        "    qc = torch.nn.utils.rnn.pad_sequence(qc, batch_first=True, padding_value=-1)\n",
        "    mask = torch.nn.utils.rnn.pad_sequence(mask, batch_first=True, padding_value=0)\n",
        "\n",
        "    return qa, qid, qc, mask"
      ],
      "metadata": {
        "id": "0msZ8OSnMvOI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df1 = pd.read_csv(\"/content/assist2015_test.csv\", header=None, sep='\\t')\n",
        "df2 = pd.read_csv(\"/content/assist2009_test.csv\", header=None, sep=',')\n",
        "print(df2.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFrML6eDJT8M",
        "outputId": "1790b5ae-3a53-4a95-ab09-277ea4f30bf4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index([0, 1], dtype='int64')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FFN(nn.Module):\n",
        "    def __init__(self, state_size=200, dropout=0.2):\n",
        "        super(FFN, self).__init__()\n",
        "        self.state_size = state_size\n",
        "        self.dropout = dropout\n",
        "        self.lr1 = nn.Linear(self.state_size, self.state_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.lr2 = nn.Linear(self.state_size, self.state_size)\n",
        "        self.dropout = nn.Dropout(self.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lr1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.lr2(x)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "yulTZX9kNAfF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SAKTLoss(nn.Module):\n",
        "    def __init__(self, reduce=\"mean\"):\n",
        "        super(SAKTLoss, self).__init__()\n",
        "        self.reduce = reduce\n",
        "\n",
        "    def forward(self, logits, targets, qid, mask, device=\"cpu\"):\n",
        "\n",
        "        mask = mask.gt(0).view(-1)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        logits = torch.masked_select(logits.view(-1), mask)\n",
        "        targets = torch.masked_select(targets, mask)\n",
        "        loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
        "            logits, targets.float(), reduction=self.reduce\n",
        "        )\n",
        "        return loss\n",
        "def dkt_predict(logits, qid):\n",
        "    preds = torch.sigmoid(logits)\n",
        "    preds = torch.gather(preds, dim=2, index=qid)\n",
        "    preds = torch.squeeze(preds)\n",
        "    binary_preds = torch.round(preds)\n",
        "    return (\n",
        "        preds.view(preds.size()[0], preds.size()[1]),\n",
        "        binary_preds.view(preds.size()[0], preds.size()[1]),\n",
        "    )"
      ],
      "metadata": {
        "id": "urLioXpuNTUS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    roc_auc_score,\n",
        "    precision_recall_fscore_support,\n",
        "    accuracy_score,\n",
        ")\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def train_epoch(model, train_iterator, optim, criterion, device=\"cpu\"):\n",
        "    model.train()\n",
        "\n",
        "    for i, (qa, qid, labels, mask) in enumerate(train_iterator):\n",
        "        qa, qid, labels, mask = (\n",
        "            qa.to(device),\n",
        "            qid.to(device),\n",
        "            labels.to(device),\n",
        "            mask.to(device),\n",
        "        )\n",
        "\n",
        "        optim.zero_grad()\n",
        "        logits, _ = model(qid, qa)\n",
        "        loss = criterion(logits, labels, qid, mask, device=device)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "\n",
        "def eval_epoch(model, test_iterator, criterion, eval_func, device=\"cpu\"):\n",
        "    model.eval()\n",
        "\n",
        "    eval_loss = []\n",
        "    preds, binary_preds, targets = [], [], []\n",
        "    for i, (qa, qid, labels, mask) in enumerate(test_iterator):\n",
        "        qa, qid, labels, mask = (\n",
        "            qa.to(device),\n",
        "            qid.to(device),\n",
        "            labels.to(device),\n",
        "            mask.to(device),\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits, _ = model(qid, qa)\n",
        "\n",
        "        loss = criterion(logits, labels, qid, mask, device=device)\n",
        "        eval_loss.append(loss.detach().item())\n",
        "\n",
        "        mask = mask.eq(1)\n",
        "\n",
        "        # pred, binary_pred = deepkt.loss.dkt_predict(logits, qid)\n",
        "        # pred = torch.masked_select(pred, mask).detach().numpy()\n",
        "        # binary_pred = torch.masked_select(binary_pred, mask).detach().numpy()\n",
        "        # target = torch.masked_select(labels, mask).detach().numpy()\n",
        "        # pred = pred.cpu().detach().numpy().reshape(-1)\n",
        "        # binary_pred = binary_pred.cpu().detach().numpy().reshape(-1)\n",
        "        pred, binary_pred, target = eval_func(logits, qid, labels, mask)\n",
        "        preds.append(pred)\n",
        "        binary_preds.append(binary_pred)\n",
        "        targets.append(target)\n",
        "\n",
        "    preds = np.concatenate(preds)\n",
        "    binary_preds = np.concatenate(binary_preds)\n",
        "    targets = np.concatenate(targets)\n",
        "\n",
        "    auc_value = roc_auc_score(targets, preds)\n",
        "    accuracy = accuracy_score(targets, binary_preds)\n",
        "    precision, recall, f_score, _ = precision_recall_fscore_support(\n",
        "        targets, binary_preds\n",
        "    )\n",
        "    pos_rate = np.sum(targets) / float(len(targets))\n",
        "    print(\n",
        "        \"auc={0}, accuracy={1}, precision={2}, recall={3}, fscore={4}, pos_rate={5}\".format(\n",
        "            auc_value, accuracy, precision, recall, f_score, pos_rate\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def dkt_eval(logits, qid, targets, mask):\n",
        "    pred, binary_pred = dkt_predict(logits, qid)\n",
        "    pred = torch.masked_select(pred, mask).detach().numpy()\n",
        "    binary_pred = torch.masked_select(binary_pred, mask).detach().numpy()\n",
        "    target = torch.masked_select(targets, mask).detach().numpy()\n",
        "    return pred, binary_pred, target\n",
        "\n",
        "\n",
        "def deepirt_eval(logits, qid, targets, mask):\n",
        "    mask = mask.gt(0).view(-1)\n",
        "    targets = targets.view(-1)\n",
        "\n",
        "    logits = torch.masked_select(logits, mask)\n",
        "\n",
        "    pred = torch.sigmoid(logits).detach().numpy()\n",
        "    binary_pred = pred.round()\n",
        "    target = torch.masked_select(targets, mask).detach().numpy()\n",
        "    return pred, binary_pred, target\n",
        "\n",
        "\n",
        "def sakt_eval(logits, qid, targets, mask):\n",
        "    mask = mask.gt(0).view(-1)\n",
        "    targets = targets.view(-1)\n",
        "\n",
        "    logits = torch.masked_select(logits.view(-1), mask)\n",
        "\n",
        "    pred = torch.sigmoid(logits).cpu().detach().numpy()\n",
        "    binary_pred = pred.round()\n",
        "    #target = torch.masked_select(targets, mask).detach().numpy()\n",
        "    target = torch.masked_select(targets, mask).cpu().detach().numpy() # Moved targets to CPU as well for consistency\n",
        "    return pred, binary_pred, target\n",
        "\n",
        "\n",
        "def future_mask(seq_length):\n",
        "    mask = np.triu(np.ones((seq_length, seq_length)), k=1).astype(\"bool\")\n",
        "    return torch.from_numpy(mask)\n"
      ],
      "metadata": {
        "id": "V_urEME-NsCS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SAKTModel(nn.Module):\n",
        "    def __init__(\n",
        "        self, n_skill, embed_dim, dropout, num_heads=4, max_len=64, device=\"cpu\"\n",
        "    ):\n",
        "        super(SAKTModel, self).__init__()\n",
        "        self.n_skill = n_skill\n",
        "        self.q_embed_dim = embed_dim\n",
        "        self.qa_embed_dim = embed_dim\n",
        "        self.pos_embed_dim = embed_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dropout = dropout\n",
        "        self.num_heads = num_heads\n",
        "        self.max_len = max_len\n",
        "        self.device = device\n",
        "\n",
        "        self.q_embedding = nn.Embedding(\n",
        "            n_skill + 1, self.q_embed_dim, padding_idx=n_skill\n",
        "        )\n",
        "        self.qa_embedding = nn.Embedding(\n",
        "            2 * n_skill + 2, self.qa_embed_dim, padding_idx=2 * n_skill + 1\n",
        "        )\n",
        "        self.pos_embedding = nn.Embedding(self.max_len, self.pos_embed_dim)\n",
        "\n",
        "        self.multi_attention = nn.MultiheadAttention(\n",
        "            embed_dim=self.embed_dim, num_heads=self.num_heads, dropout=self.dropout\n",
        "        )\n",
        "\n",
        "        self.key_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "        self.value_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "        self.query_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "        self.layer_norm1 = nn.LayerNorm(self.embed_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(self.embed_dim)\n",
        "        self.dropout_layer = nn.Dropout(self.dropout)\n",
        "        self.ffn = FFN(self.embed_dim)\n",
        "        self.pred = nn.Linear(self.embed_dim, 1, bias=True)\n",
        "\n",
        "    def forward(self, q, qa):\n",
        "        qa = self.qa_embedding(qa)\n",
        "        pos_id = torch.arange(qa.size(1)).unsqueeze(0).to(self.device)\n",
        "        pos_x = self.pos_embedding(pos_id)\n",
        "        qa = qa + pos_x\n",
        "        q = self.q_embedding(q)\n",
        "\n",
        "        q = q.permute(1, 0, 2)\n",
        "        qa = qa.permute(1, 0, 2)\n",
        "\n",
        "        attention_mask = future_mask(q.size(0)).to(self.device)\n",
        "        attention_out, _ = self.multi_attention(q, qa, qa, attn_mask=attention_mask)\n",
        "        attention_out = self.layer_norm1(attention_out + q)\n",
        "        attention_out = attention_out.permute(1, 0, 2)\n",
        "\n",
        "        x = self.ffn(attention_out)\n",
        "        x = self.dropout_layer(x)\n",
        "        x = self.layer_norm2(x + attention_out)\n",
        "        x = self.pred(x)\n",
        "\n",
        "        return x.squeeze(-1), None\n"
      ],
      "metadata": {
        "id": "qGF3bnM8N8hB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import time\n",
        "\n",
        "sys.path.insert(0, \"..\")\n",
        "\n",
        "import argparse\n",
        "import torch\n",
        "import torch.optim\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.INFO)\n",
        "\n",
        "\n",
        "def run(args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_df = pd.read_csv(\"/content/assist2009_train.csv\",\n",
        "                           header=None,\n",
        "                           sep=',')\n",
        "    test_df = pd.read_csv(\"/content/assist2009_test.csv\", header=None, sep=',')\n",
        "\n",
        "    train = SAKTDataset(train_df.reset_index(drop=True), args.num_skill, max_len=128)\n",
        "    test = SAKTDataset(test_df.reset_index(drop=True), args.num_skill, max_len=128)\n",
        "    train_dataloader = DataLoader(train,\n",
        "                                  batch_size=args.batch_size,\n",
        "                                  num_workers=args.num_worker,\n",
        "                                  shuffle=True)\n",
        "    test_dataloader = DataLoader(test,\n",
        "                                 batch_size=args.batch_size * 2,\n",
        "                                 num_workers=args.num_worker,\n",
        "                                 shuffle=False)\n",
        "\n",
        "    sakt = SAKTModel(args.num_skill, args.embed_dim, args.dropout, args.num_heads, device=device, max_len=128)\n",
        "\n",
        "    optimizer = torch.optim.Adam(sakt.parameters(), lr=args.learning_rate)\n",
        "    loss_func = SAKTLoss()\n",
        "\n",
        "    sakt.to(device)\n",
        "    loss_func.to(device)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
        "    start_time = time.time()\n",
        "    for epoch in range(args.epoch):\n",
        "        train_epoch(sakt, train_dataloader, optimizer, loss_func,\n",
        "                                 device)\n",
        "        eval_epoch(sakt, test_dataloader, loss_func, sakt_eval, device)\n",
        "        scheduler.step()\n",
        "    print(\"total time: \", time.time() - start_time, \"seconds\")\n",
        "    print('average time per epoch', (time.time() - start_time) / args.epoch)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    arg_parser = argparse.ArgumentParser(description=\"train deep IRT model\")\n",
        "    # Provide default values for the arguments\n",
        "    arg_parser.add_argument(\"--learning_rate\", dest=\"learning_rate\", default=0.001, type=float)\n",
        "    arg_parser.add_argument(\"--batch_size\", dest=\"batch_size\", default=64, type=int)\n",
        "    arg_parser.add_argument(\"--num_skill\", dest=\"num_skill\", default=150, type=int)\n",
        "    arg_parser.add_argument(\"--embed_dim\", dest=\"embed_dim\", default=100, type=int)\n",
        "    arg_parser.add_argument(\"--dropout\", dest=\"dropout\", default=0.2, type=float)\n",
        "    arg_parser.add_argument(\"--num_heads\", dest=\"num_heads\", default=10, type=int)\n",
        "    arg_parser.add_argument(\"--epoch\", dest=\"epoch\", default=20, type=int)\n",
        "    arg_parser.add_argument(\"--num_worker\", dest=\"num_worker\", default=0, type=int)\n",
        "\n",
        "    args = arg_parser.parse_args([]) #remove any other code that is being passed to parse_args such as empty lists.\n",
        "    run(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDSBntAFOeO6",
        "outputId": "4210dd92-a8be-4f44-d10d-11a312271c39"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "auc=0.7020712324346841, accuracy=0.6856903669062672, precision=[0.61123138 0.70751829], recall=[0.37989711 0.86126511], fscore=[0.46856697 0.77685781], pos_rate=0.6352587973788285\n",
            "auc=0.7201436968246349, accuracy=0.6974105828353685, precision=[0.63100888 0.71805934], recall=[0.41036803 0.86221939], fscore=[0.49731441 0.78356391], pos_rate=0.6352587973788285\n",
            "auc=0.7290742755663177, accuracy=0.7038191738113796, precision=[0.63982103 0.72460609], recall=[0.43007519 0.86099246], fscore=[0.51438849 0.78693359], pos_rate=0.6352587973788285\n",
            "auc=0.7325546624852934, accuracy=0.7050027424150573, precision=[0.67301633 0.71307712], recall=[0.37190344 0.89625557], fscore=[0.47907427 0.79424142], pos_rate=0.6352587973788285\n",
            "auc=0.73529193804431, accuracy=0.7075719523108456, precision=[0.65977803 0.72155063], recall=[0.40933914 0.87880578], fscore=[0.50522614 0.79245206], pos_rate=0.6352587973788285\n",
            "auc=0.7364707851558705, accuracy=0.7087555209145232, precision=[0.66511025 0.7212506 ], recall=[0.40585675 0.88266836], fscore=[0.5041042  0.79383697], pos_rate=0.6352587973788285\n",
            "auc=0.7380116733282189, accuracy=0.7098813544643631, precision=[0.6757308  0.71908671], recall=[0.3933518  0.89162047], fscore=[0.49724862 0.79611296], pos_rate=0.6352587973788285\n",
            "auc=0.7364598175386998, accuracy=0.7075430847839266, precision=[0.69704123 0.70990208], recall=[0.35053423 0.91252386], fscore=[0.46648059 0.79856044], pos_rate=0.6352587973788285\n",
            "auc=0.7373327330485577, accuracy=0.709910221991282, precision=[0.65098085 0.72926334], recall=[0.44123467 0.86417341], fscore=[0.52596821 0.79100722], pos_rate=0.6352587973788285\n",
            "auc=0.738523454102761, accuracy=0.7104298374758234, precision=[0.67245033 0.72101436], recall=[0.40182034 0.88762156], fscore=[0.50304682 0.79569016], pos_rate=0.6352587973788285\n",
            "auc=0.7381886657454035, accuracy=0.7110071880142028, precision=[0.66594991 0.7243314 ], recall=[0.41669964 0.87998728], fscore=[0.51263327 0.79460824], pos_rate=0.6352587973788285\n",
            "auc=0.7381406376884442, accuracy=0.710862850379608, precision=[0.65454975 0.72909661], recall=[0.43893945 0.86699082], fscore=[0.52548797 0.79208702], pos_rate=0.6352587973788285\n",
            "auc=0.7385393561586129, accuracy=0.7110360555411218, precision=[0.65796125 0.72778369], recall=[0.43268698 0.8708534 ], fscore=[0.52205882 0.79291655], pos_rate=0.6352587973788285\n",
            "auc=0.7377125211849083, accuracy=0.7115845385525822, precision=[0.66996657 0.72363474], recall=[0.4124258 0.88335  ], fscore=[0.510557   0.79555546], pos_rate=0.6352587973788285\n",
            "auc=0.7388357821324479, accuracy=0.7109783204872838, precision=[0.67414686 0.72120989], recall=[0.40182034 0.88848496], fscore=[0.50352078 0.79615604], pos_rate=0.6352587973788285\n",
            "auc=0.7388392599766204, accuracy=0.7126526370485841, precision=[0.68240577 0.7207973 ], recall=[0.39691334 0.89393802], fscore=[0.50190152 0.79808512], pos_rate=0.6352587973788285\n",
            "auc=0.7384992386686208, accuracy=0.7121907566178806, precision=[0.65420669 0.73146154], recall=[0.44740799 0.86421885], fscore=[0.53139688 0.79231763], pos_rate=0.6352587973788285\n",
            "auc=0.7386735714521286, accuracy=0.7125371669409082, precision=[0.6641727  0.72742374], recall=[0.42857143 0.87557939], fscore=[0.52097364 0.79465501], pos_rate=0.6352587973788285\n",
            "auc=0.7387247860343589, accuracy=0.7117288761871771, precision=[0.66658282 0.72517797], recall=[0.41946973 0.87953285], fscore=[0.51491305 0.79493182], pos_rate=0.6352587973788285\n",
            "auc=0.7390883574185132, accuracy=0.7117577437140961, precision=[0.66435128 0.72613718], recall=[0.42390186 0.87703354], fscore=[0.51756293 0.79448389], pos_rate=0.6352587973788285\n",
            "total time:  20.74994945526123 seconds\n",
            "average time per epoch 1.0374982118606568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "import torch.nn as nn\n",
        "\n",
        "def measure_memory(model, input_q, input_qa):\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    model.cuda()\n",
        "    input_q = input_q.cuda()\n",
        "    input_qa = input_qa.cuda()\n",
        "\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(input_q, input_qa)\n",
        "\n",
        "    max_mem = torch.cuda.max_memory_allocated() / (1024 ** 2)  # in MB\n",
        "    print(f\"Max memory used: {max_mem:.2f} MB\")\n",
        "    return max_mem\n",
        "\n",
        "# Define parameters for the SAKTModel instance\n",
        "n_skill = 150  # Example value, adjust based on your data\n",
        "embed_dim = 100\n",
        "dropout = 0.001\n",
        "num_heads = 10\n",
        "max_len = 128 # Max sequence length\n",
        "batch_size = 64 # Use a small batch size for memory measurement\n",
        "\n",
        "# Create an instance of SAKTModel\n",
        "# You also need to provide device if not default\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "sakt_model_instance = SAKTModel(n_skill, embed_dim, dropout, num_heads, max_len=max_len, device=device)\n",
        "\n",
        "# Create dummy input tensors with appropriate dimensions (batch_size, sequence_length)\n",
        "# The SAKTModel forward method expects q and qa as inputs.\n",
        "# q is typically the question sequence, qa is the question-answer sequence.\n",
        "# The values should be within the range of your embedding layers' vocabulary size.\n",
        "# For q, values should be < n_skill + 1.\n",
        "# For qa, values should be < 2 * n_skill + 2.\n",
        "dummy_input_q = torch.randint(0, n_skill, (batch_size, max_len), dtype=torch.long)\n",
        "dummy_input_qa = torch.randint(0, 2 * n_skill + 1, (batch_size, max_len), dtype=torch.long)\n",
        "\n",
        "\n",
        "# Pass the instance and dummy inputs to the measure_memory function\n",
        "mem_dot = measure_memory(sakt_model_instance, dummy_input_q, dummy_input_qa)\n",
        "\n",
        "# Correct the typo in print\n",
        "print(mem_dot)"
      ],
      "metadata": {
        "id": "pveAfYqlWjqm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c69b680-6825-4724-b4d7-a66096ba6e43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max memory used: 126.77 MB\n",
            "126.765625\n"
          ]
        }
      ]
    }
  ]
}